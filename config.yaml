# LLM Adapter Configuration
# Environment variables can be referenced using ${VAR_NAME} syntax

llm:
  default_provider: dashscope
  # Global default generation parameters (applied to all providers/models)
  default_generation_params:
    temperature: 0.7
    max_tokens: 2048
    top_p: 0.9

providers:
  openai:
    api_key: 
    base_url: https://api.openai.com
    models:
      cheap: gpt-4o-mini
      normal: gpt-4o
      premium: gpt-4-turbo
      multimodal: gpt-4o
    # Provider-level generation params (overrides global defaults)
    generation_params:
      temperature: 0.8
      presence_penalty: 0.1
      frequency_penalty: 0.1
    # Model-specific params (overrides provider-level)
    model_params:
      gpt-4o-mini:
        temperature: 0.5
        max_tokens: 1024
      gpt-4-turbo:
        temperature: 0.9
        max_tokens: 4096

  gemini:
    api_key: REDACTED_GOOGLE_API_KEY
    mode: vertex  # Options: "http" (default), "sdk", "vertex"
    # Vertex AI specific settings (only needed when mode: vertex)
    ## project_id 在 json密钥里
    project_id: wingy-e87ee
    location: asia-southeast1
    models:
      cheap: gemini-2.5-flash
      normal: gemini-2.5-flash
      premium: gemini-2.5-flash
      multimodal: gemini-2.5-flash
    # Gemini-specific generation params
    generation_params:
      temperature: 0.7
      top_p: 0.95
      top_k: 40
      max_tokens: 2048

  # cloudflare:
  #   api_key: 
  #   account_id: ${CF_ACCOUNT_ID}
  #   models:
  #     cheap: "@cf/meta/llama-3-8b-instruct"
  #     normal: "@cf/meta/llama-3-8b-instruct"
  #     multimodal: "@cf/meta/llama-3-8b-instruct"

  huggingface:
    api_key: 
    default_model: meta-llama/Llama-3.1-8B-Instruct
    models:
      cheap: meta-llama/Llama-3.1-8B-Instruct
      normal: meta-llama/Llama-3.1-8B-Instruct
      multimodal: meta-llama/Llama-3.1-8B-Instruct

  dashscope:
    api_key: ${DASHSCOPY_API_KEY}
    base_url: ${DASHSCOPY_BASE_URL}
    models:
      cheap: qwen-flash
      normal: qwen-flash
      premium: qwen-flash
      multimodal: qwen-vl-plus
    # DashScope-specific params
    generation_params:
      temperature: 0.7
      top_p: 0.8
      seed: 1234  # For reproducible results

  openrouter:
    api_key: ${OPENROUTER_API_KEY}
    base_url: ${OPENROUTER_BASE_URL}
    models:
      cheap: google/gemini-2.0-flash-001
      normal: google/gemini-2.5-flash
      premium: google/gemini-3-flash-preview
      multimodal: qwen/qwen3-vl-30b-a3b-instruct
    # OpenRouter-specific params
    generation_params:
      temperature: 0.75
      max_tokens: 3000
    model_params:
      google/gemini-3-flash-preview:
        temperature: 0.9
        max_tokens: 4096

pricing:
  openai:
    gpt-4o-mini:
      input_cost_per_1m: 0.15
      output_cost_per_1m: 0.60
    gpt-4o:
      input_cost_per_1m: 2.50
      output_cost_per_1m: 10.00
    gpt-4-turbo:
      input_cost_per_1m: 10.00
      output_cost_per_1m: 30.00

  gemini:
    gemini-1.5-flash:
      input_cost_per_1m: 0.075
      output_cost_per_1m: 0.30
    gemini-1.5-pro:
      input_cost_per_1m: 1.25
      output_cost_per_1m: 5.00

  cloudflare:
    "@cf/meta/llama-3-8b-instruct":
      input_cost_per_1m: 0.00
      output_cost_per_1m: 0.00

  huggingface:
    meta-llama/Llama-3.1-8B-Instruct:
      input_cost_per_1m: 0.00
      output_cost_per_1m: 0.00

  dashscope:
    qwen-turbo:
      input_cost_per_1m: 0.30
      output_cost_per_1m: 0.60
    qwen-plus:
      input_cost_per_1m: 0.80
      output_cost_per_1m: 2.00
    qwen-max:
      input_cost_per_1m: 2.00
      output_cost_per_1m: 6.00

  openrouter:
    meta-llama/llama-3.1-8b-instruct:
      input_cost_per_1m: 0.055
      output_cost_per_1m: 0.055
    anthropic/claude-3.5-sonnet:
      input_cost_per_1m: 3.00
      output_cost_per_1m: 15.00
    anthropic/claude-3-opus:
      input_cost_per_1m: 15.00
      output_cost_per_1m: 75.00

profile:
  trait_rules:
    min_confidence_to_count: 0.7


proxy:
  enable: false
  host: http://proxy.zhizitech.org
  port: 10803

# HTTP Client Configuration (Optional)
# Controls connection pool and timeout settings for all adapters
# http_client:
#   max_connections: 100              # Maximum concurrent connections (default: 100)
#   max_keepalive_connections: 20    # Keep-alive connections for reuse (default: 20)
#   timeout: 60.0                     # Request timeout in seconds (default: 60.0)